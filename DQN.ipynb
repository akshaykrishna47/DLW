{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gym) (1.26.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gym) (3.0.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gym) (0.0.8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install gym\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from gym import spaces\n",
    "\n",
    "class ThermostatEnvironment(gym.Env):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(ThermostatEnvironment, self).__init__()\n",
    "        self.day = 1\n",
    "        self.time = 1\n",
    "        self.booking_status = 0\n",
    "        self.outside_temp = 30\n",
    "        self.inside_temp = 18\n",
    "        self.action_space = spaces.Discrete(3) #increase, decrease, maintain\n",
    "        self.observation_space = spaces.Box(low=np.array([1,1,0,23,18]), high=np.array([5,24,1,40,25]), dtype=np.float32)\n",
    "        self.set_point = 18\n",
    "        self.temperature = 18\n",
    "        self.max_temp = 25\n",
    "        self.min_temp = 18\n",
    "        self.reward = 0\n",
    "        self.done = False\n",
    "    \n",
    "    def step(self, action):\n",
    "        #action logic\n",
    "        if action == \"0\": #decrease\n",
    "            self.temperature -=1\n",
    "        elif action == \"2\": #increase\n",
    "            self.temperature +=1\n",
    "        else: #maintain\n",
    "            self.temperature = self.temperature\n",
    "\n",
    "        \n",
    "        # ensure within range\n",
    "        if self.temperature < self.min_temp:\n",
    "            self.temperature = self.min_temp\n",
    "        elif self.temperature > self.max_temp:\n",
    "            self.temperature = self.max_temp\n",
    "\n",
    "        # reward logic\n",
    "        # if energy consumption is high, reward is low\n",
    "        # if energy consumption is low, reward is high\n",
    "        # if energy consumption is optimal, reward is highest\n",
    "        \n",
    "\n",
    "    def reset(self):\n",
    "        self.temperature = 18\n",
    "        self.day = 1\n",
    "        self.time = 1\n",
    "        self.booking_status = 0\n",
    "        self.outside_temperature = 18\n",
    "        return self.temperature\n",
    "    \n",
    "    def action_space(self):\n",
    "        return self.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_size, action_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 25)  # 5 input features, 1 output features\n",
    "        self.fc2 = nn.Linear(25,25)\n",
    "        self.fc3 = nn.Linear(25, action_size) # 10 input features, 2 output features\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQN(\n",
      "  (fc1): Linear(in_features=5, out_features=25, bias=True)\n",
      "  (fc2): Linear(in_features=25, out_features=25, bias=True)\n",
      "  (fc3): Linear(in_features=25, out_features=3, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gym\\spaces\\box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    }
   ],
   "source": [
    "#test the model\n",
    "env = ThermostatEnvironment()\n",
    "model = DQN(5,3)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self.build_model()\n",
    "\n",
    "\n",
    "    #return action based on state\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            act_values = self.model(state)\n",
    "        return np.argmax(act_values[0])\n",
    "    \n",
    "    # remember experience\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            state = torch.FloatTensor(state).unsqueeze(0)\n",
    "            next_state = torch.FloatTensor(next_state).unsqueeze(0)\n",
    "            target = reward\n",
    "            if not done:\n",
    "                with torch.no_grad():\n",
    "                    target = (reward + self.gamma * torch.max(self.model(next_state)[0])).item()\n",
    "            target_f = self.model(state)\n",
    "            target_f[0][action] = target\n",
    "            self.optimizer.zero_grad()\n",
    "            loss = nn.MSELoss()(target_f, self.model(state))\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def build_model(self):\n",
    "        self.model = DQN(self.state_size, self.action_size)  # Ensure this matches your DQN class\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        return self.model\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
