{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal: Use DQN model to setup optimal temperature for a room <br>\n",
    "\n",
    "Factors affecting initial temperature: <br>\n",
    "<ol>\n",
    "    <li> Room booking status </li>\n",
    "    <li> Outside Temp</li>\n",
    "    <li> Day </li>\n",
    "    <li> Time </li>\n",
    "</ol>\n",
    "\n",
    "Action taken by DQN: <br>\n",
    "<ol>\n",
    "    <li> Increase Temperature </li>\n",
    "    <li> Decrease Temperature   </li>\n",
    "    <li> Maintain Temperature </li>\n",
    "</ol>\n",
    "\n",
    "Rewards for DQN : <br>\n",
    "<ol>\n",
    "    <li> Energy Consumption Levels</li>\n",
    "    <li> User Comfort</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 591.7480183967483\n",
      "Percentage Error: 0.5739767899029191\n"
     ]
    }
   ],
   "source": [
    "# Machine Learning to predict energy consumption\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "training_data = pd.read_excel('output.xlsx')\n",
    "# for day data change mon to 1, tue to 2 etc\n",
    "\n",
    "training_data['day_'] = training_data['day_'].replace(['Mon','Tue','Wed','Thu','Fri','Sat','Sun'],[1,2,3,4,5,6,7])\n",
    "features = ['day_', 'time_', 'outside_temp', 'inside_temp', 'booking_status']\n",
    "X = training_data[features]\n",
    "y = training_data['energy_consumption']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Random forest model to predict energy consumption\n",
    "\n",
    "rf_model = RandomForestRegressor(n_estimators=200, random_state= 50) # 200 trees, random_state for reproducibility\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred = rf_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "# percentage error\n",
    "percentage_error = (np.mean(np.abs((y_test - y_pred) / y_test)) * 100)\n",
    "print(f\"Percentage Error: {percentage_error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equation to calculate energy consumption\n",
    "def energy_consumption(day, time, outside_temp, inside_temp, booking_status, change_in_temp):\n",
    "    day = day\n",
    "    time = time\n",
    "    outside_temp = outside_temp\n",
    "    inside_temp = inside_temp\n",
    "    booking_status = booking_status\n",
    "    change_in_temp = change_in_temp\n",
    "    energy_consumption = rf_model.predict([[day, time, outside_temp, inside_temp, booking_status]])\n",
    "    return energy_consumption\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from gym import spaces\n",
    "class ThermostatEnvironment(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(ThermostatEnvironment, self).__init__()\n",
    "        self.day = 1\n",
    "        self.time = 1\n",
    "        self.booking_status = 0\n",
    "        self.outside_temp = 30\n",
    "        self.inside_temp = 18 # what room temperature should be\n",
    "        self.action_space = spaces.Discrete(3) #increase, decrease, maintain\n",
    "        self.observation_space = spaces.Box(low=np.array([1,1,0,23,18]), high=np.array([5,24,1,40,25]), dtype=np.float32)\n",
    "        self.temperature = 18 # what room temperature is\n",
    "        self.max_temp = 25\n",
    "        self.min_temp = 18\n",
    "        self.reward = 0\n",
    "        self.done = False\n",
    "    def step(self, action):\n",
    "        # action logic\n",
    "        if action == 0: # decrease, too hot\n",
    "            self.temperature -= 1\n",
    "        elif action == 2: #increase, too cold\n",
    "            self.temperature += 1\n",
    "        else: #maintain\n",
    "            pass\n",
    "\n",
    "        # ensure temperature is within bounds\n",
    "        self.temperature = np.clip(self.temperature, self.min_temp, self.max_temp)\n",
    "\n",
    "        # calculate energy consumption\n",
    "        energy = energy_consumption(self.day, self.time, self.outside_temp, self.inside_temp, self.booking_status, self.temperature - self.inside_temp)\n",
    "\n",
    "        # calculate reward\n",
    "        # reward = 0.6* energy reward + 0.4* comfort reward\n",
    "        # energy reward\n",
    "        energy_reward = - (energy - 3200)/3200 # 3200 is the average energy consumption\n",
    "        print(f\"Energy Reward: {energy_reward}\")\n",
    "\n",
    "        # comfort reward\n",
    "        # reward if user does not change temperature\n",
    "        change_in_temp = self.temperature - self.inside_temp\n",
    "        if change_in_temp == 0:\n",
    "            comfort_reward = 1\n",
    "        if change_in_temp > 0:\n",
    "            comfort_reward = -0\n",
    "        if change_in_temp < 0:\n",
    "            comfort_reward = -1\n",
    "        print(f\"Comfort Reward: {comfort_reward}\")\n",
    "\n",
    "        reward = 0.6* energy_reward + 0.4* comfort_reward\n",
    "        print(f\"Total Reward: {reward}\")\n",
    "        self.reward += reward\n",
    "        self.done = True\n",
    "\n",
    "        return np.array([self.day, self.time, self.booking_status, self.outside_temp, self.inside_temp]), reward, self.done, {}\n",
    "    \n",
    "    def reset(self):\n",
    "        self.day = 1\n",
    "        self.time = 1\n",
    "        self.booking_status = 0\n",
    "        self.outside_temp = 30\n",
    "        self.inside_temp = 18\n",
    "        self.temperature = 18\n",
    "        self.reward = 0\n",
    "        self.done = False\n",
    "        return np.array([self.day, self.time, self.booking_status, self.outside_temp, self.inside_temp])\n",
    "    \n",
    "    def render(self):\n",
    "        pass\n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import copy\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_size, action_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 25)  # 5 input features, 1 output features\n",
    "        self.fc2 = nn.Linear(25,25)\n",
    "        self.fc3 = nn.Linear(25, action_size) # 10 input features, 2 output features\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# memory\n",
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "    \n",
    "    def push(self, state, action, next_state, reward):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append((state, action, next_state, reward))\n",
    "        else:\n",
    "            self.memory.pop(0)\n",
    "            self.memory.append((state, action, next_state, reward))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gym\\spaces\\box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.001\n",
    "gamma = 0.9\n",
    "buffer_limit = 50000\n",
    "batch_size = 32\n",
    "tau = 0.01\n",
    "input_size = 5\n",
    "action_size = 4\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize the environment\n",
    "env = ThermostatEnvironment()\n",
    "env.reset()\n",
    "\n",
    "# Initialize the model\n",
    "model = DQN(input_size, action_size).to(device)\n",
    "target_model = copy.deepcopy(model)\n",
    "target_model.load_state_dict(model.state_dict())\n",
    "target_model.eval()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Initialize the replay buffer\n",
    "replay_buffer = []\n",
    "replay_buffer_size = 0\n",
    "\n",
    "# Initialize the training parameters\n",
    "if torch.cuda.is_available():\n",
    "    print('Using GPU')\n",
    "    num_episodes = 500\n",
    "else:\n",
    "    print('Using CPU')\n",
    "    num_episodes = 100\n",
    "epsilon = 0.1\n",
    "epsilon_decay = 0.99\n",
    "epsilon_min = 0.01\n",
    "update_every = 10\n",
    "update_count = 0\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "# Get number of states from gym observation space\n",
    "state,info = env.reset(),{}\n",
    "n_states = len(state)\n",
    "\n",
    "policy_net = DQN(n_states, n_actions).to(device)\n",
    "target_net = DQN(n_states, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=0.01)\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "def select_action(state, epsilon):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = epsilon\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)\n",
    "\n",
    "episodes_duration = []\n",
    "\n",
    "def optimize_model():\n",
    "    if len(memory) < batch_size:\n",
    "        return\n",
    "    transitions = memory.sample(batch_size)\n",
    "    batch = memory.sample(batch_size)\n",
    "\n",
    "    state_batch, action_batch, next_state_batch, reward_batch = zip(*batch)\n",
    "    state_batch = torch.cat(state_batch)\n",
    "    next_state_batch = torch.cat(next_state_batch)\n",
    "    reward_batch = torch.cat(reward_batch)\n",
    "    action_batch = torch.tensor(action_batch).to(device)\n",
    "\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "    next_state_values = target_net(next_state_batch).max(1)[0].detach()\n",
    "    expected_state_action_values = (next_state_values * gamma) + reward_batch\n",
    "\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Total Reward: -0.14783568680286407, Loss: None\n",
      "Episode 1, Total Reward: 0.2521643042564392, Loss: None\n",
      "Episode 2, Total Reward: 0.2521643042564392, Loss: None\n",
      "Episode 3, Total Reward: 0.2521643042564392, Loss: None\n",
      "Episode 4, Total Reward: 0.2521643042564392, Loss: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 5, Total Reward: 0.2521643042564392, Loss: None\n",
      "Episode 6, Total Reward: 0.2521643042564392, Loss: None\n",
      "Episode 7, Total Reward: 0.2521643042564392, Loss: None\n",
      "Episode 8, Total Reward: 0.2521643042564392, Loss: None\n",
      "Episode 9, Total Reward: 0.2521643042564392, Loss: None\n",
      "Episode 10, Total Reward: 0.2521643042564392, Loss: None\n",
      "Episode 11, Total Reward: -0.14783568680286407, Loss: None\n",
      "Episode 12, Total Reward: 0.2521643042564392, Loss: None\n",
      "Episode 13, Total Reward: 0.2521643042564392, Loss: None\n",
      "Episode 14, Total Reward: 0.2521643042564392, Loss: None\n",
      "Episode 15, Total Reward: 0.2521643042564392, Loss: None\n",
      "Episode 16, Total Reward: 0.2521643042564392, Loss: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 17, Total Reward: 0.2521643042564392, Loss: None\n",
      "Episode 18, Total Reward: 0.2521643042564392, Loss: None\n",
      "Episode 19, Total Reward: 0.2521643042564392, Loss: None\n",
      "Episode 20, Total Reward: 0.2521643042564392, Loss: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 21, Total Reward: 0.2521643042564392, Loss: None\n",
      "Episode 22, Total Reward: 0.2521643042564392, Loss: None\n",
      "Episode 23, Total Reward: 0.2521643042564392, Loss: None\n",
      "Episode 24, Total Reward: 0.2521643042564392, Loss: None\n",
      "Episode 25, Total Reward: 0.2521643042564392, Loss: None\n",
      "Episode 26, Total Reward: 0.2521643042564392, Loss: None\n",
      "Episode 27, Total Reward: 0.2521643042564392, Loss: None\n",
      "Episode 28, Total Reward: 0.2521643042564392, Loss: None\n",
      "Episode 29, Total Reward: 0.2521643042564392, Loss: None\n",
      "Episode 30, Total Reward: 0.2521643042564392, Loss: None\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Index tensor must have the same number of dimensions as input tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m     state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[0;32m     16\u001b[0m     total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m---> 17\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43moptimize_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     19\u001b[0m episodes_duration\u001b[38;5;241m.\u001b[39mappend(steps)\n",
      "Cell \u001b[1;32mIn[36], line 81\u001b[0m, in \u001b[0;36moptimize_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m     78\u001b[0m reward_batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(reward_batch)\n\u001b[0;32m     79\u001b[0m action_batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(action_batch)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 81\u001b[0m state_action_values \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_batch\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgather\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m next_state_values \u001b[38;5;241m=\u001b[39m target_net(next_state_batch)\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[0;32m     83\u001b[0m expected_state_action_values \u001b[38;5;241m=\u001b[39m (next_state_values \u001b[38;5;241m*\u001b[39m gamma) \u001b[38;5;241m+\u001b[39m reward_batch\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Index tensor must have the same number of dimensions as input tensor"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    state = torch.tensor([state], device=device, dtype=torch.float32)\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    while not done:\n",
    "        action = select_action(state, epsilon)\n",
    "        next_state, reward, done, _ = env.step(action.item())\n",
    "        next_state = torch.tensor([next_state], device=device, dtype=torch.float32)\n",
    "        reward = torch.tensor([reward], device=device, dtype=torch.float32)\n",
    "        memory.push(state, action, next_state, reward)\n",
    "        state = next_state\n",
    "        total_reward += reward.item()\n",
    "        loss = optimize_model()\n",
    "        steps += 1\n",
    "    episodes_duration.append(steps)\n",
    "    print(f\"Episode {episode}, Total Reward: {total_reward}, Loss: {loss}\")\n",
    "    if episode % update_every == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "    epsilon = max(epsilon_min, epsilon*epsilon_decay)\n",
    "\n",
    "# Plot the duration of each episode\n",
    "plt.plot(episodes_duration)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Duration')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
